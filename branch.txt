This document should exist in every branch, albeit with different contents.
It should describe the development priorities of the branch, as well as brainstorming ideas
throughout the development process.


Priorities:

1. Move towards release - merge in outstanding branches.

2. Help setup SAS case with the one established correct current GITR version.

3. Get that test case running on fusiont6 and producing useful research results as well as
   verifying correct operation of the software.

4. Set up a harder test case - the multi-species one. Dimes problem? 
   Start with concrete expected correct research results.
   Then work backwards through post-processing, to GITR expected outputs, until we meet the
   furthest edge of the brainstorming section below.

   Visualization of development workflow:

    path_0: START HERE ( design notions ) -----> ( concrete design plan ) ---> DESTINATION 

    path_1: START HERE ( define test results ) ---> ( define test parameters ) ---> DESTINATION   
    design_path: path_0_start -----> DESTINATION <----- path_1_start

    From start to finish, design notions are fundamentally connected to concrete, well-defined
    test results that serve as a clear measure of success.

5. Once a clear design_path has been established, begin defining subtasks and time estimates.



Design Notions for multi-species design plan:

This is brainstorming. What is the basic "shape" of the new solution?


Terminology section for New surface model, subject to change, needs better terms:

Micro-timesteps: 

each step of movement along each particle's
independent timeline. Different for every particle, allows
for adaptive timestepping.

Macro-timesteps: 

each set of particles re-initialized after
each particle in the set has experienced another impact event or the original
set of particles re-initialized after the 0th impact event
(starting the simulation)

Currently, GITR maps from pre-impact state, to impact state, to post-impact state.
Proposed change is to have GITR map only pre-impact state to impact state. This can be done
twice in succession to get equivalent behavior without the memory overhead of storing surface
modeling results inside GITR.

A new module will need to be created called "GITR_surface" that facilitates this pattern by
storing lists of impact events and converting them into re-initialization events to initialize
the next state:

while( 1 )
{

  < initialize particles according to (re)initialization event list >

  < collect impact events >

  particle 0: { energy, angle, impact coordinates, particle species }
  particle 1: { ... }
  particle 2: { ... }
  particle 3: { ... }
  particle 4: { ... }
  .
  .
  .
  .

  < decide when to pause >

  < send to GITR_surface module >

  converts the list to re-initialization events:

  particle 0: { new energy, new angle, impact coordinate, new particle species }
  particle 1: { ... }
  particle 2: { ... }
  particle 3: { ... }
  particle 4: { ... }

  < send back to GITR_transport >

}

Questions:

  0. Performance. How will performance be preserved? Adding this separation in the state mapping
     incurs overhead. How can that overhead be made worth it?


  1. How do we know when to pause GITR and exchange an impact event list
     for a re-initialization event list? Parameters involved in the choice:

     variables:

     n_particles = particles in the simulation

     | impact_event_list | = length of the impact_event_list

     n_reinitialize = integer value, when | impact_event_list | == n_reinitialize, pause
     and convert each impact event to a reinitialization event and reinitialize.

     since one particle can make one impact, invariants:

     | impact_event_list | <= n_particles


     | impact_event_list | == | reinitialization_event_list |

     There are 2 parts to this question:

     A. Build the infrastructure to allow conversion from impact to re-initialiation.

     B. Given the infrastructure, how frequently should the simulation be paused to allow
        for a reinitialization step? How should n_reinitialize be picked?

  2. Investigating part B above:

    n_reinitialize = 1 = most accurate results, terrible performance
    n_reinitialize = n_particles = low accuracy, best performance.

    characterize the tradeoff mathematically:


  We lose the relative order of impacts:


  n_particles = 13 

  symbol for pause/reinitialize = |

  real time impact times:

  0  1  2  3  4  5  6  7  8  9  10  11  12

  n_reinitialization = 1

  0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12

  n_reinitializeation = 5

  0 1 2 3 4 5 | 6 7 8 9 10 | 11 12

  relative order of impacts is lost in between '| |'

  We could call this the "monte-carlo sampling rate" or something.

  What can this cause? The particles that impacted at times 0 1 2 3 4 5 effectivly impact
  the surface at the same time. This is in contrast to what is truly happening physically.

  What are the limits of this approximation? How could we quantify them effectively?

  Certainly, some pipelining will have to be done to achieve any level of good performance.

  This means, while one GITR module is converting impact events to reinit events, another
  module needs to be moving around particles that have not yet impacted.








